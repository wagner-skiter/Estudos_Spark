{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "matched-gentleman",
   "metadata": {},
   "source": [
    "# Analisando dados do Uber Utilizando o Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binary-window",
   "metadata": {},
   "source": [
    "DataSet: https://github.com/fivethirtyeight/uber-tlc-foil-response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "quality-annex",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "actual-prison",
   "metadata": {},
   "outputs": [],
   "source": [
    "uberFile = read_csv('C:/Users/skite/OneDrive/Documentos/GitHub/spark_dados_uber/uber.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "engaging-wagner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dispatching_base_number</th>\n",
       "      <th>date</th>\n",
       "      <th>active_vehicles</th>\n",
       "      <th>trips</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B02512</td>\n",
       "      <td>1/1/2015</td>\n",
       "      <td>190</td>\n",
       "      <td>1132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B02765</td>\n",
       "      <td>1/1/2015</td>\n",
       "      <td>225</td>\n",
       "      <td>1765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B02764</td>\n",
       "      <td>1/1/2015</td>\n",
       "      <td>3427</td>\n",
       "      <td>29421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B02682</td>\n",
       "      <td>1/1/2015</td>\n",
       "      <td>945</td>\n",
       "      <td>7679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B02617</td>\n",
       "      <td>1/1/2015</td>\n",
       "      <td>1228</td>\n",
       "      <td>9537</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dispatching_base_number      date  active_vehicles  trips\n",
       "0                  B02512  1/1/2015              190   1132\n",
       "1                  B02765  1/1/2015              225   1765\n",
       "2                  B02764  1/1/2015             3427  29421\n",
       "3                  B02682  1/1/2015              945   7679\n",
       "4                  B02617  1/1/2015             1228   9537"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uberFile.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acoustic-legislature",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 354 entries, 0 to 353\n",
      "Data columns (total 4 columns):\n",
      " #   Column                   Non-Null Count  Dtype \n",
      "---  ------                   --------------  ----- \n",
      " 0   dispatching_base_number  354 non-null    object\n",
      " 1   date                     354 non-null    object\n",
      " 2   active_vehicles          354 non-null    int64 \n",
      " 3   trips                    354 non-null    int64 \n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 11.2+ KB\n"
     ]
    }
   ],
   "source": [
    "uberFile.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "national-directory",
   "metadata": {},
   "source": [
    "**Vamos transformar o Dataframe (Pandas) e um Dataframe (Spark)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dietary-surgeon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\opt\\\\spark\\\\spark-3.1.1-bin-hadoop2.7'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "enclosed-explosion",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.rdd import RDD\n",
    "\n",
    "conf = pyspark.SparkConf().setAppName('SparkApp').setMaster('local[*]')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "sc._conf.get('spark.driver.memory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "challenging-billy",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "uberDF = sqlContext.createDataFrame(uberFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fatty-archives",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(uberDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overhead-concrete",
   "metadata": {},
   "source": [
    "**Após verificamos que a comunicação entre pandas e spark está funcionando vamos criar criar uma RDD apartir do arquivo .csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "neutral-contrary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uberRDD = sc.textFile('C:/Users/skite/OneDrive/Documentos/GitHub/spark_dados_uber/uber.csv')\n",
    "type(uberRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "initial-innocent",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "355"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uberRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "excited-monkey",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dispatching_base_number,date,active_vehicles,trips'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uberRDD.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlimited-platform",
   "metadata": {},
   "source": [
    "**Dividindo o arquivo em coulunas, seperadas pelo caracter ','**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ongoing-director",
   "metadata": {},
   "outputs": [],
   "source": [
    "uberLinhas = uberRDD.map(lambda line: line.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "velvet-stick",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(uberLinhas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recovered-restriction",
   "metadata": {},
   "source": [
    "**Note que após a transformação da split nossa RDD agora é uma Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "competent-violence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['dispatching_base_number', 'date', 'active_vehicles', 'trips'],\n",
       " ['B02512', '1/1/2015', '190', '1132'],\n",
       " ['B02765', '1/1/2015', '225', '1765'],\n",
       " ['B02764', '1/1/2015', '3427', '29421'],\n",
       " ['B02682', '1/1/2015', '945', '7679'],\n",
       " ['B02617', '1/1/2015', '1228', '9537'],\n",
       " ['B02598', '1/1/2015', '870', '6903'],\n",
       " ['B02598', '1/2/2015', '785', '4768'],\n",
       " ['B02617', '1/2/2015', '1137', '7065'],\n",
       " ['B02512', '1/2/2015', '175', '875']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uberLinhas.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "great-poland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uberLinhas.map(lambda linha: linha[0]).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "respiratory-coordinate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dispatching_base_number',\n",
       " 'B02765',\n",
       " 'B02682',\n",
       " 'B02598',\n",
       " 'B02512',\n",
       " 'B02764',\n",
       " 'B02617']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uberLinhas.map(lambda linha: linha[0]).distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offshore-medicine",
   "metadata": {},
   "source": [
    "**Testando o filtro de um elementos na coluna base**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "purple-opinion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uberLinhas.filter(lambda linha: 'B02617' in linha).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "opponent-miniature",
   "metadata": {},
   "outputs": [],
   "source": [
    "B02617_RDD = uberLinhas.filter(lambda linha: 'B02617' in linha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frozen-wesley",
   "metadata": {},
   "source": [
    "**Fazendo um filtro na coluna trips**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "antique-packaging",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B02617_RDD.filter(lambda linha: int(linha[3]) > 15000).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "respective-cattle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['B02617', '1/31/2015', '1394', '15756'],\n",
       " ['B02617', '2/6/2015', '1526', '15417'],\n",
       " ['B02617', '2/13/2015', '1590', '16996'],\n",
       " ['B02617', '2/14/2015', '1486', '16999'],\n",
       " ['B02617', '2/20/2015', '1574', '16856'],\n",
       " ['B02617', '2/21/2015', '1443', '16098']]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B02617_RDD.filter(lambda linha: int(linha[3]) > 15000).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federal-relay",
   "metadata": {},
   "source": [
    "**Agora vamos realizar algumas operações de MapReduce que é muito comum no Hadoop, mas que o Spark consegue fazer com mais rapidez e eficiencia.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "labeled-world",
   "metadata": {},
   "outputs": [],
   "source": [
    "uberRDD2 = sc.textFile('C:/Users/skite/OneDrive/Documentos/GitHub/spark_dados_uber/uber.csv').filter(lambda line: 'base' not in line).map(lambda line:line.split(','))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civic-procurement",
   "metadata": {},
   "source": [
    "**Criando um novo DataFrame com ele vamos remover a primeira linha que é o cabeçario.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "spread-spider",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('B02765', 193670),\n",
       " ('B02682', 662509),\n",
       " ('B02598', 540791),\n",
       " ('B02512', 93786),\n",
       " ('B02764', 1914449),\n",
       " ('B02617', 725025)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uberRDD2.map(lambda kp: (kp[0], int(kp[3]))).reduceByKey(lambda k,v: k + v).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incomplete-exploration",
   "metadata": {},
   "source": [
    "**Note que agora para cada uma das base tem a sua quantidade de viagens (trips)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "multiple-forth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('B02764', 1914449),\n",
       " ('B02617', 725025),\n",
       " ('B02682', 662509),\n",
       " ('B02598', 540791),\n",
       " ('B02765', 193670),\n",
       " ('B02512', 93786)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uberRDD2.map(lambda kp: (kp[0], int(kp[3]))).reduceByKey(lambda k,v: k + v).takeOrdered(10, key = lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romance-egyptian",
   "metadata": {},
   "source": [
    "**Podemos usar a função TakeOrdered para fazer um Map reduce a diferença entre elas é que o take ordered nos retorna o resultado de forma ordenada, ou seja, em ordem crescente.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
