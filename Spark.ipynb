{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "collective-rwanda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\opt\\\\spark\\\\spark-3.1.1-bin-hadoop2.7'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "conceptual-disability",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = pyspark.SparkConf().setAppName('SparkApp').setMaster('local')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "better-crime",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'pyspark' from 'C:\\\\opt\\\\spark\\\\spark-3.1.1-bin-hadoop2.7\\\\python\\\\pyspark\\\\__init__.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "first-belarus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testando o Spark e criando uma RDD\n",
    "\n",
    "lst = [25, 90, 81, 37, 776, 3320]\n",
    "testdata = sc.parallelize(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "strange-apple",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumSlices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Distribute a local Python collection to form an RDD. Using range\n",
       "is recommended if the input represents a range for performance.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> sc.parallelize([0, 2, 3, 4, 6], 5).glom().collect()\n",
       "[[0], [2], [3], [4], [6]]\n",
       ">>> sc.parallelize(range(0, 6, 2), 5).glom().collect()\n",
       "[[], [0], [], [2], [4]]\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\opt\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\pyspark\\context.py\n",
       "\u001b[1;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?sc.parallelize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "reflected-wisdom",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(testdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "surprising-daniel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdata.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "sharp-photography",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[25, 90, 81, 37, 776, 3320]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdata.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "editorial-gallery",
   "metadata": {},
   "source": [
    "**RDD's são coleçães distribuídas de itens. RDD's podem ser criadas a partir do Hadoop (Arquivos no HDFS), através da transformação de outras RDD's a partir de bancos de dados(relacionais e não-relacionais) ou a partir de arquivos locais.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "configured-appendix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando uma RDD a partir de um arquivo csv\n",
    "\n",
    "sentimentoRDD = sc.textFile('sentimentos.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "packed-sheet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sentimentoRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "modern-spirituality",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ação - contando o número de registros\n",
    "\n",
    "sentimentoRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "possible-israel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # listando os 5 primeiros registros\n",
    "\n",
    "sentimentoRDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rising-isaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformando os dados - transformação letras maiúsculas\n",
    "\n",
    "transfRDD = sentimentoRDD.map(lambda x: x.upper())\n",
    "transfRDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "circular-recycling",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentimentoRDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elementary-suggestion",
   "metadata": {},
   "outputs": [],
   "source": [
    "arquivo = sc.textFile('sentimentos.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resident-injury",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(arquivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wired-canal",
   "metadata": {},
   "outputs": [],
   "source": [
    "arquivo.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparative-portrait",
   "metadata": {},
   "outputs": [],
   "source": [
    "arquivo.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "theoretical-plumbing",
   "metadata": {},
   "outputs": [],
   "source": [
    "linhasComSol = arquivo.filter(lambda line:'Sol' in line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competent-diversity",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(linhasComSol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "committed-black",
   "metadata": {},
   "outputs": [],
   "source": [
    "linhasComSol.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forbidden-scale",
   "metadata": {},
   "outputs": [],
   "source": [
    "linhasComSol.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radio-negative",
   "metadata": {},
   "source": [
    "Primeiro a função **map()** determina o comprimento de cada linha do arquivo, criando uma RDD. A função **reduce()** é chamada para encontar a linha com maior número de caracteres. o argumento para as funções **map() e reduce()** são funções anônimas criadas com lambda(da linguagem Python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signed-trace",
   "metadata": {},
   "outputs": [],
   "source": [
    "arquivo.map(lambda line: len(line.split())).reduce(lambda a, b: a if (a > b) else b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "square-community",
   "metadata": {},
   "source": [
    "Esta linha pode ser reescrita da seguinte forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absent-project",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max(a, b):\n",
    "    if a > b:\n",
    "        return a\n",
    "    else:\n",
    "        return b\n",
    "\n",
    "arquivo.map(lambda line: len(line.split())).reduce(max)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silent-safety",
   "metadata": {},
   "source": [
    "## Operação de MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geological-doctor",
   "metadata": {},
   "source": [
    "As operações de MapReduce foram popularizadas pelo hadoop e podem ser feitas com Spark até 100x mais rápiado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annoying-heater",
   "metadata": {},
   "outputs": [],
   "source": [
    "contaPalavras = arquivo.flatMap(lambda line : linesplit()).map(lambda palavra: (palavra, 1)).reduceByKey(lambda a, b:a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emerging-capture",
   "metadata": {},
   "outputs": [],
   "source": [
    "contaPalavras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beneficial-pontiac",
   "metadata": {},
   "source": [
    "http://localhost:4040/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
